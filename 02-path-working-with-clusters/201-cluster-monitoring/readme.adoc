= Kubernetes Cluster Monitoring
:toc:
:icons:
:linkcss:
:imagesdir: ../../resources/images

== Introduction

This chapter will demonstrate how to monitor a Kubernetes cluster using the following:

. Kubernetes Dashboard
. Heapster
. Prometheus, Node exporter and Grafana

http://prometheus.io/[Prometheus] is an open-source systems monitoring and alerting toolkit. Prometheus collects metrics from monitored targets by scraping metrics from HTTP endpoints on these targets.

Heapster is limited to Kuberenetes container metrics, it is not general use. Heapster can be used as Prometheus scrape target.

== Prerequisites

In order to perform exercises in this chapter, youâ€™ll need to deploy configurations to a Kubernetes cluster. To create a Kubernetes cluster, use link:../../01-path-basics/102-your-first-cluster[kops].

== Kubernetes Dashboard

https://github.com/kubernetes/dashboard[Kubernetes Dashboard] is a general purpose web-based UI for Kubernetes clusters.

The Dashboard uses the https://kubernetes.io/docs/admin/authorization/rbac/[RBAC API], which has been promoted in
Kubernetes v1.8 to GA rather than Beta. 

If you are using v1.8 or above, deploy the Dashboard using the following command:

  $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/kubernetes-dashboard/v1.8.3.yaml

Dashboard can be seen using the following command:

  $ kubectl proxy  

Now, Dashboard is accessible via below url:

    http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/


Starting with Kubernetes 1.7, Dashboard supports authentication. Read more about it at https://github.com/kubernetes/dashboard/wiki/Access-control#introduction. We'll use a bearer token for authentication.

create one service account to access the dashboard

    # Create the service account in the current namespace 
    # (we assume default)
    kubectl create serviceaccount my-dashboard-sa
    # Give that service account root on the cluster
    kubectl create clusterrolebinding my-dashboard-sa \
      --clusterrole=cluster-admin \
      --serviceaccount=default:my-dashboard-sa
    # Find the secret that was created to hold the token for the SA
    kubectl get secrets
    # Show the contents of the secret to extract the token
    kubectl describe secret my-dashboard-sa-token-xxxxx


Copy the value of token from this output, select `Token` in the Dashboard login window, and paste the text. Click on `SIGN IN` to see the default Dashboard view:

image::kubernetes-dashboard-default.png[]

Click on `Nodes` to see a textual representation about the nodes running in the cluster:

image::monitoring-nodes-before.png[]


== Heapster

https://github.com/kubernetes/heapster[Heapster] is a metrics aggregator and processor. It is installed as a cluster-wide pod. It gathers monitoring and events data for all containers on each node by talking to the Kubelet. Kubelet itself fetches this data from https://github.com/google/cadvisor[cAdvisor]. 

Heapster collects and interprets various signals like compute resource usage, lifecycle events, etc., and exports cluster metrics via REST endpoints.

Heapster is http://kubernetes.io/docs/admin/addons/[Kubernetes addons].

=== Installation

Execute this command to install Heapster:

  $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/kops/master/addons/monitoring-standalone/v1.7.0.yaml

Heapster is now aggregating metrics from the cAdvisor instances running on each node.  

After the deployment of Heapster, Kubernetes Dashboard now shows additional graphs such as CPU and Memory utilization for pods and nodes, and other workloads.

The updated view of the cluster in Kubernetes Dashboard looks like this:

image::monitoring-nodes-after.png[]

The updated view of pods looks like this:

image::monitoring-pods-after.png[]


== Prometheus, Node exporter and Grafana

http://prometheus.io/[Prometheus] is an open-source systems monitoring and alerting toolkit. Prometheus collects metrics from monitored targets by scraping metrics from HTTP endpoints on these targets.

Prometheus will be managed by the https://github.com/coreos/prometheus-operator/[Kubernetes Operator] - This operator uses https://kubernetes.io/docs/concepts/api-extension/custom-resources/[Custom Resources] to extend the Kubernetes API and add custom resources such as `Prometheus`, `ServiceMonitor` and `Alertmanager`.

Prometheus is able to dynamically scrape new targets by adding a https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/running-exporters.md[ServiceMonitor] - we have included a couple of them to scrape `kube-controller-manager`, `kube-scheduler`, `kube-state-metrics`, `kubelet` and `node-exporter`.

https://github.com/prometheus/node_exporter[Node exporter] is a Prometheus exporter for hardware and OS metrics exposed by *NIX kernels.
https://github.com/kubernetes/kube-state-metrics[kube-state-metrics] is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.

=== Installation

First we need to deploy the Prometheus Operator which will listen for the new Custom Resources:

  $ kubectl apply -f templates/prometheus/v0.19.0.yaml

Next we need to wait until the Prometheus Operator has started:


  $ kubectl rollout status deployment/prometheus-operator -n monitoring
  ...
  deployment "prometheus-operator" successfully rolled out

Lets wait for prometheus to come up:

  $ kubectl get po -l app=prometheus -n monitoring
  NAME                      READY     STATUS    RESTARTS   AGE
  prometheus-prometheus-0   2/2       Running   0          1m
  prometheus-prometheus-1   2/2       Running   0          1m

=== Prometheus Dashboard

Prometheus is now scraping metrics from the different scraping targets and we forward the dashboard via:

  $ kubectl port-forward $(kubectl get po -l app=prometheus -n monitoring -o jsonpath={.items[0].metadata.name}) 9090 -n monitoring
  Forwarding from 127.0.0.1:9090 -> 9090

Now open the browser at http://localhost:9090/targets and all targets should be shown as `UP` (it might take a couple of minutes until data collectors are up and running for the first time). The browser displays the output as shown:

image::monitoring-grafana-prometheus-dashboard-1.png[]
image::monitoring-grafana-prometheus-dashboard-2.png[]
image::monitoring-grafana-prometheus-dashboard-3.png[]

=== Grafana Dashboard

Lets forward the grafana dashboard to a local port:

  $ kubectl port-forward $(kubectl get pod -l app=grafana -o jsonpath={.items[0].metadata.name} -n monitoring) 3000 -n monitoring
  Forwarding from 127.0.0.1:3000 -> 3000

Grafana dashboard is now accessible at http://localhost:3000/. Default login is admin/admin. The complete list of dashboards is available using the search button at the top:

image::monitoring-grafana-prometheus-dashboard-dashboard-home.png[]

You can access various metrics using these dashboards:

. http://localhost:3000/dashboard/db/kubernetes-control-plane-status?orgId=1[Kubernetes Cluster Control Plane]
+
image::monitoring-grafana-prometheus-dashboard-control-plane-status.png[]
+
. http://localhost:3000/dashboard/db/kubernetes-cluster-status?orgId=1[Kubernetes Cluster Status]
+
image::monitoring-grafana-prometheus-dashboard-cluster-status.png[]
+
. http://localhost:3000/dashboard/db/kubernetes-capacity-planning?orgId=1[Kubernetes Cluster Capacity Planning]
+
image::monitoring-grafana-prometheus-dashboard-capacity-planning.png[]
+
. http://localhost:3000/dashboard/db/nodes?orgId=1[Nodes in the Kubernetes cluster]
+
image::monitoring-grafana-prometheus-dashboard-nodes.png[]

Convenient link for other dashboards are listed below:

* http://localhost:3000/dashboard/db/deployment&orgId=1
* http://localhost:3000/dashboard/db/kubernetes-cluster-health?refresh=10s&orgId=1
* http://localhost:3000/dashboard/db/kubernetes-resource-requests?orgId=1
* http://localhost:3000/dashboard/db/pods?orgId=1


You are now ready to continue on with the workshop!

:frame: none
:grid: none
:valign: top

[align="center", cols="2", grid="none", frame="none"]
|=====
|image:button-continue-standard.png[link=../../02-path-working-with-clusters/202-service-mesh]
|image:button-continue-operations.png[link=../../02-path-working-with-clusters/202-service-mesh]
|link:../../standard-path.adoc[Go to Standard Index]
|link:../../operations-path.adoc[Go to Operations Index]
|=====
